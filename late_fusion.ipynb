{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGPGJacVIA7b",
        "outputId": "1c62c2a6-a3ed-404b-f839-7868a50d973c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktE0VJe72Tv8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
        "\n",
        "# 1. Load Pose Keypoints CSV\n",
        "pose_csv = \"/content/drive/MyDrive/HRNet-Human-Pose-Estimation/tools/casia_pose_all_subjects.csv\"  # your path\n",
        "df = pd.read_csv(pose_csv)\n",
        "\n",
        "# 2. Extract Image Names and Keypoints\n",
        "image_names = df['image'].tolist()\n",
        "keypoints = df.iloc[:, 1:].values.reshape(-1, 17, 2)\n",
        "\n",
        "# 3. Build Graphs\n",
        "COCO_EDGES = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
        "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
        "    (5, 11), (6, 12), (11, 12), (11, 13), (13, 15),\n",
        "    (12, 14), (14, 16)\n",
        "]\n",
        "edge_index = torch.tensor(COCO_EDGES, dtype=torch.long).t().contiguous()\n",
        "pose_graphs = [Data(x=torch.tensor(kpt, dtype=torch.float), edge_index=edge_index) for kpt in keypoints]\n",
        "\n",
        "# 4. Label Encoding\n",
        "subject_ids = [img.split('/')[0] for img in image_names]\n",
        "le = LabelEncoder()\n",
        "encoded_labels = le.fit_transform(subject_ids)\n",
        "label_map = {sid: lbl for sid, lbl in zip(subject_ids, encoded_labels)}\n",
        "\n",
        "# 5. Fusion Dataset\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, root_dir, image_names, pose_graphs, label_map, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_names = image_names\n",
        "        self.pose_graphs = pose_graphs\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform or transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      rel_path = self.image_names[idx]\n",
        "      full_path = os.path.join(self.root_dir, rel_path)\n",
        "\n",
        "      image = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
        "      if image is None:\n",
        "          raise FileNotFoundError(f\"Image not found or cannot be read: {full_path}\")\n",
        "\n",
        "      image = cv2.resize(image, (128, 128))\n",
        "      image = self.transform(image)\n",
        "      graph = self.pose_graphs[idx]\n",
        "\n",
        "      subject_id = rel_path.split('/')[0]\n",
        "      label = torch.tensor(self.label_map[subject_id], dtype=torch.long)\n",
        "      return image, graph, label\n",
        "\n",
        "\n",
        "# 6. Create Dataset and Split\n",
        "dataset = FusionDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/Dataset\",  # update this path\n",
        "    image_names=image_names,\n",
        "    pose_graphs=pose_graphs,\n",
        "    label_map=label_map\n",
        ")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# 7. Create Dataloaders\n",
        "train_loader = GeoDataLoader(train_set, batch_size=16, shuffle=True)\n",
        "val_loader = GeoDataLoader(val_set, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "# CNN Backbone (e.g., ResNet18 without final FC)\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=128):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # For grayscale\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC\n",
        "        self.fc = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # (B, 512, 1, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# GCN Encoder\n",
        "class GCNEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=128):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(2, 64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.fc = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Late Fusion Classifier\n",
        "class LateFusionModel(nn.Module):\n",
        "    def __init__(self, cnn_out=128, gcn_out=128, num_classes=124):\n",
        "        super(LateFusionModel, self).__init__()\n",
        "        self.cnn = CNNEncoder(cnn_out)\n",
        "        self.gcn = GCNEncoder(gcn_out)\n",
        "        self.classifier = nn.Linear(cnn_out + gcn_out, num_classes)\n",
        "\n",
        "    def forward(self, image, graph_data):\n",
        "        img_feat = self.cnn(image)\n",
        "        gcn_feat = self.gcn(graph_data)\n",
        "        combined = torch.cat([img_feat, gcn_feat], dim=1)\n",
        "        return self.classifier(combined)\n"
      ],
      "metadata": {
        "id": "iJDOPgzrIzm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LateFusionModel(num_classes=len(set(encoded_labels))).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, graphs, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        graphs = graphs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, graphs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, graphs, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            graphs = graphs.to(device)\n",
        "            outputs = model(images, graphs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 21):\n",
        "    train_loss = train(model, train_loader)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrqdcMSjI6XT",
        "outputId": "62f2a0e9-a84e-4cae-8090-c8ae22be833f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=0.6880, Val Acc=0.1351\n",
            "Epoch 2: Train Loss=0.1244, Val Acc=0.9009\n",
            "Epoch 3: Train Loss=0.0718, Val Acc=0.9640\n",
            "Epoch 4: Train Loss=0.0303, Val Acc=0.9820\n",
            "Epoch 5: Train Loss=0.0635, Val Acc=0.9910\n",
            "Epoch 6: Train Loss=0.0294, Val Acc=1.0000\n",
            "Epoch 7: Train Loss=0.0244, Val Acc=1.0000\n",
            "Epoch 8: Train Loss=0.0461, Val Acc=0.9009\n",
            "Epoch 9: Train Loss=0.0199, Val Acc=0.9910\n",
            "Epoch 10: Train Loss=0.0090, Val Acc=0.9910\n",
            "Epoch 11: Train Loss=0.0090, Val Acc=0.9730\n",
            "Epoch 12: Train Loss=0.0061, Val Acc=1.0000\n",
            "Epoch 13: Train Loss=0.0040, Val Acc=0.9730\n",
            "Epoch 14: Train Loss=0.0227, Val Acc=0.9730\n",
            "Epoch 15: Train Loss=0.0172, Val Acc=0.9640\n",
            "Epoch 16: Train Loss=0.0100, Val Acc=0.9550\n",
            "Epoch 17: Train Loss=0.0258, Val Acc=1.0000\n",
            "Epoch 18: Train Loss=0.0214, Val Acc=0.9820\n",
            "Epoch 19: Train Loss=0.0520, Val Acc=0.9550\n",
            "Epoch 20: Train Loss=0.0485, Val Acc=0.9459\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}