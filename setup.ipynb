{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-tM4XdyofDf",
        "outputId": "84620b87-60a5-4624-a293-6b3d629d577e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'HRNet-Human-Pose-Estimation'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 140 (delta 0), reused 1 (delta 0), pack-reused 137 (from 1)\u001b[K\n",
            "Receiving objects: 100% (140/140), 1.69 MiB | 9.20 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "/content/HRNet-Human-Pose-Estimation\n",
            "Collecting EasyDict==1.7 (from -r requirements.txt (line 1))\n",
            "  Downloading easydict-1.7.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.17.61, 4.4.0.42, 4.4.0.44, 4.5.4.58, 4.5.5.62, 4.7.0.68\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==3.4.1.15 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84, 4.11.0.86)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==3.4.1.15\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting yacs==0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs==0.1.8) (6.0.2)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install dependencies and setup repo\n",
        "%cd /content\n",
        "!rm -rf HRNet-Human-Pose-Estimation  # Clean slate\n",
        "!git clone https://github.com/HRNet/HRNet-Human-Pose-Estimation.git\n",
        "%cd HRNet-Human-Pose-Estimation\n",
        "\n",
        "# Install required packages\n",
        "!pip install -r requirements.txt\n",
        "!pip install yacs==0.1.8\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "# Add HRNet and lib to Python path\n",
        "import sys\n",
        "sys.path.append('/content/HRNet-Human-Pose-Estimation/lib')\n",
        "sys.path.append('/content/HRNet-Human-Pose-Estimation')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Download model weights to expected folder\n",
        "!mkdir -p models/pytorch/pose_coco/\n",
        "!wget https://huggingface.co/Prophetetc/cocopose/resolve/main/pose_hrnet_w48_384x288.pth -O models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLTHJNI8pCl5",
        "outputId": "8e78a4b5-47c7-4088-eb4d-d3772864a1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 18:06:35--  https://huggingface.co/Prophetetc/cocopose/resolve/main/pose_hrnet_w48_384x288.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.124, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/40/d8/40d8bd0880e5d15def6352e0f9a2a0d7a3322a394071fd4f98cc17da1ad50c33/95e0fec3194826d5e3f806ea89be68bbb84517b114c3a32b3058c56610b5ef61?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pose_hrnet_w48_384x288.pth%3B+filename%3D%22pose_hrnet_w48_384x288.pth%22%3B&Expires=1745348795&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTM0ODc5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQwL2Q4LzQwZDhiZDA4ODBlNWQxNWRlZjYzNTJlMGY5YTJhMGQ3YTMzMjJhMzk0MDcxZmQ0Zjk4Y2MxN2RhMWFkNTBjMzMvOTVlMGZlYzMxOTQ4MjZkNWUzZjgwNmVhODliZTY4YmJiODQ1MTdiMTE0YzNhMzJiMzA1OGM1NjYxMGI1ZWY2MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=J-eU-%7EFPr1wAzO3yUIwYaqy7PyHGag1bOpnlTF4SSPI9autuXPU8FpG4qDyAH3-UmNijEa26466ktXuObAeJuQeCDSO8T0bnsQuAmln9QdZ1eK%7EobFftzawbv%7EoTzwXrJGND0iO-3AyqTLVRMHciBWVZkgmPKhG5R-V30TN520T-CazcrMVv0QbE7VPGKOdUVM7QwqkIJEAR1JCwWQu-zO4CqLTgOepD08siTs2LsaxMTCYiV2fIqp59A8tcvGJK3O5jiiJAu88Y3yi5ys084FHVm1kfD9L%7Eu-zh6D%7Ewr8D2nx5U6T1uCbJuIkMVU9WL%7ERWWYPu-LR4AZfY0JctwXQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-22 18:06:35--  https://cdn-lfs-us-1.hf.co/repos/40/d8/40d8bd0880e5d15def6352e0f9a2a0d7a3322a394071fd4f98cc17da1ad50c33/95e0fec3194826d5e3f806ea89be68bbb84517b114c3a32b3058c56610b5ef61?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pose_hrnet_w48_384x288.pth%3B+filename%3D%22pose_hrnet_w48_384x288.pth%22%3B&Expires=1745348795&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTM0ODc5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQwL2Q4LzQwZDhiZDA4ODBlNWQxNWRlZjYzNTJlMGY5YTJhMGQ3YTMzMjJhMzk0MDcxZmQ0Zjk4Y2MxN2RhMWFkNTBjMzMvOTVlMGZlYzMxOTQ4MjZkNWUzZjgwNmVhODliZTY4YmJiODQ1MTdiMTE0YzNhMzJiMzA1OGM1NjYxMGI1ZWY2MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=J-eU-%7EFPr1wAzO3yUIwYaqy7PyHGag1bOpnlTF4SSPI9autuXPU8FpG4qDyAH3-UmNijEa26466ktXuObAeJuQeCDSO8T0bnsQuAmln9QdZ1eK%7EobFftzawbv%7EoTzwXrJGND0iO-3AyqTLVRMHciBWVZkgmPKhG5R-V30TN520T-CazcrMVv0QbE7VPGKOdUVM7QwqkIJEAR1JCwWQu-zO4CqLTgOepD08siTs2LsaxMTCYiV2fIqp59A8tcvGJK3O5jiiJAu88Y3yi5ys084FHVm1kfD9L%7Eu-zh6D%7Ewr8D2nx5U6T1uCbJuIkMVU9WL%7ERWWYPu-LR4AZfY0JctwXQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.160.213.31, 18.160.213.54, 18.160.213.113, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.160.213.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255061287 (243M) [binary/octet-stream]\n",
            "Saving to: ‘models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth’\n",
            "\n",
            "models/pytorch/pose 100%[===================>] 243.25M   112MB/s    in 2.2s    \n",
            "\n",
            "2025-04-22 18:06:37 (112 MB/s) - ‘models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth’ saved [255061287/255061287]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Upload sample images\n",
        "from google.colab import files\n",
        "import os, shutil\n",
        "\n",
        "os.makedirs('demo_images', exist_ok=True)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to demo_images\n",
        "for fname in uploaded.keys():\n",
        "    shutil.move(fname, os.path.join('demo_images', fname))\n",
        "\n",
        "# Check they are there\n",
        "!ls demo_images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W8vKm2_fpHHU",
        "outputId": "9af9b240-bbc6-4c01-9d3e-655d0db0dc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-acb5e706-4bb7-4d36-bf8b-7d2796377985\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-acb5e706-4bb7-4d36-bf8b-7d2796377985\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 062-nm-06-180-072.png to 062-nm-06-180-072.png\n",
            "Saving 062-nm-06-180-073.png to 062-nm-06-180-073.png\n",
            "Saving 062-nm-06-180-074.png to 062-nm-06-180-074.png\n",
            "Saving 062-nm-06-180-075.png to 062-nm-06-180-075.png\n",
            "Saving 062-nm-06-180-076.png to 062-nm-06-180-076.png\n",
            "Saving 062-nm-06-180-077.png to 062-nm-06-180-077.png\n",
            "Saving 062-nm-06-180-078.png to 062-nm-06-180-078.png\n",
            "Saving 062-nm-06-180-079.png to 062-nm-06-180-079.png\n",
            "Saving 062-nm-06-180-080.png to 062-nm-06-180-080.png\n",
            "Saving 062-nm-06-180-081.png to 062-nm-06-180-081.png\n",
            "Saving 062-nm-06-180-082.png to 062-nm-06-180-082.png\n",
            "Saving 062-nm-06-180-083.png to 062-nm-06-180-083.png\n",
            "Saving 062-nm-06-180-084.png to 062-nm-06-180-084.png\n",
            "Saving 062-nm-06-180-085.png to 062-nm-06-180-085.png\n",
            "Saving 062-nm-06-180-086.png to 062-nm-06-180-086.png\n",
            "Saving 062-nm-06-180-087.png to 062-nm-06-180-087.png\n",
            "Saving 062-nm-06-180-088.png to 062-nm-06-180-088.png\n",
            "Saving 062-nm-06-180-089.png to 062-nm-06-180-089.png\n",
            "Saving 062-nm-06-180-090.png to 062-nm-06-180-090.png\n",
            "Saving 062-nm-06-180-091.png to 062-nm-06-180-091.png\n",
            "Saving 062-nm-06-180-092.png to 062-nm-06-180-092.png\n",
            "Saving 062-nm-06-180-093.png to 062-nm-06-180-093.png\n",
            "Saving 062-nm-06-180-094.png to 062-nm-06-180-094.png\n",
            "Saving 062-nm-06-180-095.png to 062-nm-06-180-095.png\n",
            "Saving 062-nm-06-180-096.png to 062-nm-06-180-096.png\n",
            "Saving 062-nm-06-180-097.png to 062-nm-06-180-097.png\n",
            "Saving 062-nm-06-180-098.png to 062-nm-06-180-098.png\n",
            "Saving 062-nm-06-180-099.png to 062-nm-06-180-099.png\n",
            "Saving 062-nm-06-180-100.png to 062-nm-06-180-100.png\n",
            "Saving 062-nm-06-180-101.png to 062-nm-06-180-101.png\n",
            "Saving 062-nm-06-180-102.png to 062-nm-06-180-102.png\n",
            "Saving 062-nm-06-180-103.png to 062-nm-06-180-103.png\n",
            "Saving 062-nm-06-180-104.png to 062-nm-06-180-104.png\n",
            "Saving 062-nm-06-180-105.png to 062-nm-06-180-105.png\n",
            "Saving 062-nm-06-180-106.png to 062-nm-06-180-106.png\n",
            "Saving 062-nm-06-180-107.png to 062-nm-06-180-107.png\n",
            "Saving 062-nm-06-180-108.png to 062-nm-06-180-108.png\n",
            "Saving 062-nm-06-180-109.png to 062-nm-06-180-109.png\n",
            "Saving 062-nm-06-180-110.png to 062-nm-06-180-110.png\n",
            "Saving 062-nm-06-180-111.png to 062-nm-06-180-111.png\n",
            "Saving 062-nm-06-180-112.png to 062-nm-06-180-112.png\n",
            "Saving 062-nm-06-180-113.png to 062-nm-06-180-113.png\n",
            "Saving 062-nm-06-180-114.png to 062-nm-06-180-114.png\n",
            "Saving 062-nm-06-180-115.png to 062-nm-06-180-115.png\n",
            "Saving 062-nm-06-180-116.png to 062-nm-06-180-116.png\n",
            "Saving 062-nm-06-180-117.png to 062-nm-06-180-117.png\n",
            "Saving 062-nm-06-180-118.png to 062-nm-06-180-118.png\n",
            "Saving 062-nm-06-180-041.png to 062-nm-06-180-041.png\n",
            "Saving 062-nm-06-180-042.png to 062-nm-06-180-042.png\n",
            "Saving 062-nm-06-180-043.png to 062-nm-06-180-043.png\n",
            "Saving 062-nm-06-180-044.png to 062-nm-06-180-044.png\n",
            "Saving 062-nm-06-180-045.png to 062-nm-06-180-045.png\n",
            "Saving 062-nm-06-180-046.png to 062-nm-06-180-046.png\n",
            "Saving 062-nm-06-180-047.png to 062-nm-06-180-047.png\n",
            "Saving 062-nm-06-180-048.png to 062-nm-06-180-048.png\n",
            "Saving 062-nm-06-180-049.png to 062-nm-06-180-049.png\n",
            "Saving 062-nm-06-180-050.png to 062-nm-06-180-050.png\n",
            "Saving 062-nm-06-180-051.png to 062-nm-06-180-051.png\n",
            "Saving 062-nm-06-180-052.png to 062-nm-06-180-052.png\n",
            "Saving 062-nm-06-180-053.png to 062-nm-06-180-053.png\n",
            "Saving 062-nm-06-180-054.png to 062-nm-06-180-054.png\n",
            "Saving 062-nm-06-180-055.png to 062-nm-06-180-055.png\n",
            "Saving 062-nm-06-180-056.png to 062-nm-06-180-056.png\n",
            "Saving 062-nm-06-180-057.png to 062-nm-06-180-057.png\n",
            "Saving 062-nm-06-180-058.png to 062-nm-06-180-058.png\n",
            "Saving 062-nm-06-180-059.png to 062-nm-06-180-059.png\n",
            "Saving 062-nm-06-180-060.png to 062-nm-06-180-060.png\n",
            "Saving 062-nm-06-180-061.png to 062-nm-06-180-061.png\n",
            "Saving 062-nm-06-180-062.png to 062-nm-06-180-062.png\n",
            "Saving 062-nm-06-180-063.png to 062-nm-06-180-063.png\n",
            "Saving 062-nm-06-180-064.png to 062-nm-06-180-064.png\n",
            "Saving 062-nm-06-180-065.png to 062-nm-06-180-065.png\n",
            "Saving 062-nm-06-180-066.png to 062-nm-06-180-066.png\n",
            "Saving 062-nm-06-180-067.png to 062-nm-06-180-067.png\n",
            "Saving 062-nm-06-180-068.png to 062-nm-06-180-068.png\n",
            "Saving 062-nm-06-180-069.png to 062-nm-06-180-069.png\n",
            "Saving 062-nm-06-180-070.png to 062-nm-06-180-070.png\n",
            "Saving 062-nm-06-180-071.png to 062-nm-06-180-071.png\n",
            "Saving 062-nm-06-180-040.png to 062-nm-06-180-040.png\n",
            "062-nm-06-180-040.png  062-nm-06-180-067.png  062-nm-06-180-094.png\n",
            "062-nm-06-180-041.png  062-nm-06-180-068.png  062-nm-06-180-095.png\n",
            "062-nm-06-180-042.png  062-nm-06-180-069.png  062-nm-06-180-096.png\n",
            "062-nm-06-180-043.png  062-nm-06-180-070.png  062-nm-06-180-097.png\n",
            "062-nm-06-180-044.png  062-nm-06-180-071.png  062-nm-06-180-098.png\n",
            "062-nm-06-180-045.png  062-nm-06-180-072.png  062-nm-06-180-099.png\n",
            "062-nm-06-180-046.png  062-nm-06-180-073.png  062-nm-06-180-100.png\n",
            "062-nm-06-180-047.png  062-nm-06-180-074.png  062-nm-06-180-101.png\n",
            "062-nm-06-180-048.png  062-nm-06-180-075.png  062-nm-06-180-102.png\n",
            "062-nm-06-180-049.png  062-nm-06-180-076.png  062-nm-06-180-103.png\n",
            "062-nm-06-180-050.png  062-nm-06-180-077.png  062-nm-06-180-104.png\n",
            "062-nm-06-180-051.png  062-nm-06-180-078.png  062-nm-06-180-105.png\n",
            "062-nm-06-180-052.png  062-nm-06-180-079.png  062-nm-06-180-106.png\n",
            "062-nm-06-180-053.png  062-nm-06-180-080.png  062-nm-06-180-107.png\n",
            "062-nm-06-180-054.png  062-nm-06-180-081.png  062-nm-06-180-108.png\n",
            "062-nm-06-180-055.png  062-nm-06-180-082.png  062-nm-06-180-109.png\n",
            "062-nm-06-180-056.png  062-nm-06-180-083.png  062-nm-06-180-110.png\n",
            "062-nm-06-180-057.png  062-nm-06-180-084.png  062-nm-06-180-111.png\n",
            "062-nm-06-180-058.png  062-nm-06-180-085.png  062-nm-06-180-112.png\n",
            "062-nm-06-180-059.png  062-nm-06-180-086.png  062-nm-06-180-113.png\n",
            "062-nm-06-180-060.png  062-nm-06-180-087.png  062-nm-06-180-114.png\n",
            "062-nm-06-180-061.png  062-nm-06-180-088.png  062-nm-06-180-115.png\n",
            "062-nm-06-180-062.png  062-nm-06-180-089.png  062-nm-06-180-116.png\n",
            "062-nm-06-180-063.png  062-nm-06-180-090.png  062-nm-06-180-117.png\n",
            "062-nm-06-180-064.png  062-nm-06-180-091.png  062-nm-06-180-118.png\n",
            "062-nm-06-180-065.png  062-nm-06-180-092.png\n",
            "062-nm-06-180-066.png  062-nm-06-180-093.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Create hrnet_infer.py\n",
        "code = '''\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(__file__), 'lib'))\n",
        "\n",
        "from config.default import _C as cfg\n",
        "from config.default import update_config\n",
        "from models.pose_hrnet import get_pose_net\n",
        "from core.inference import get_final_preds\n",
        "from utils.transforms import get_affine_transform\n",
        "\n",
        "def preprocess_image(image, input_size):\n",
        "    h, w = image.shape[:2]\n",
        "    center = np.array([w / 2, h / 2], dtype=np.float32)\n",
        "    scale = max(h, w) / 200\n",
        "    trans = get_affine_transform(center, scale, 0, input_size)\n",
        "    input_img = cv2.warpAffine(image, trans, input_size, flags=cv2.INTER_LINEAR)\n",
        "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
        "    input_img = input_img.astype(np.float32) / 255.0\n",
        "    input_img = input_img.transpose(2, 0, 1)\n",
        "    input_img = torch.from_numpy(input_img).unsqueeze(0)\n",
        "    return input_img, center, scale\n",
        "\n",
        "def extract_keypoints(model, image_path, input_size=(384,288)):\n",
        "    image = cv2.imread(image_path)\n",
        "    input_tensor, center, scale = preprocess_image(image, input_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        preds, _ = get_final_preds(cfg, outputs.clone().cpu().numpy(), [center], [scale])\n",
        "    return preds[0]  # 17 keypoints [x, y]\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--cfg', required=True)\n",
        "    parser.add_argument('--model-file', required=True)\n",
        "    parser.add_argument('--image-dir', required=True)\n",
        "    parser.add_argument('--save-path', default='keypoints.csv')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # ✅ Add missing expected args\n",
        "    args.modelDir = ''\n",
        "    args.logDir = ''\n",
        "    args.dataDir = ''\n",
        "    args.opts = []\n",
        "\n",
        "    update_config(cfg, args)\n",
        "\n",
        "    model = get_pose_net(cfg, is_train=False)\n",
        "    model.load_state_dict(torch.load(args.model_file, map_location='cpu'))\n",
        "    model.eval()\n",
        "\n",
        "    image_paths = sorted(glob.glob(os.path.join(args.image_dir, '*.png')))\n",
        "    all_keypoints = []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        keypoints = extract_keypoints(model, img_path)\n",
        "        row = [os.path.basename(img_path)] + keypoints.flatten().tolist()\n",
        "        all_keypoints.append(row)\n",
        "\n",
        "    columns = ['image'] + [f'{joint}_{coord}' for joint in range(17) for coord in ['x', 'y']]\n",
        "    df = pd.DataFrame(all_keypoints, columns=columns)\n",
        "    df.to_csv(args.save_path, index=False)\n",
        "    print(f\"✅ Saved keypoints to {args.save_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('hrnet_infer.py', 'w') as f:\n",
        "    f.write(code)\n",
        "print(\"✅ hrnet_infer.py created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5WPvTjLpSxA",
        "outputId": "8e565bcf-b8da-4fa4-a74e-386f9ad32cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ hrnet_infer.py created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python hrnet_infer.py \\\n",
        "  --cfg experiments/coco/hrnet/w48_384x288_adam_lr1e-3.yaml \\\n",
        "  --model-file models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n",
        "  --image-dir demo_images \\\n",
        "  --save-path casia_pose_output.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_coXap9QpWiw",
        "outputId": "d193fe63-e9b4-4242-bb95-7ae5b0d0fc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HRNet-Human-Pose-Estimation/lib/models/pose_hrnet.py:487: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  or self.pretrained_layers[0] is '*':\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "0.32\n",
            "✅ Saved keypoints to casia_pose_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the keypoints CSV\n",
        "df = pd.read_csv('casia_pose_output.csv')\n",
        "\n",
        "# Remove filename column\n",
        "keypoints_data = df.iloc[:, 1:].values  # shape: [N, 34] (17 joints * 2)\n",
        "\n",
        "# Reshape to [frames, 17 joints, 2 coords]\n",
        "keypoints_sequence = keypoints_data.reshape(len(df), 17, 2)\n",
        "\n",
        "print(\"Shape:\", keypoints_sequence.shape)\n",
        "# Optional: Save as numpy array for GCN\n",
        "np.save(\"casia_pose_sequence.npy\", keypoints_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVkh1n7krsqF",
        "outputId": "c2ea2bc0-0a45-4022-da11-b703a11f419b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (79, 17, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch Geometric (official recommendation for Colab)\n",
        "# First install torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
        "# Then torch-geometric\n",
        "\n",
        "# 1. Install core dependencies\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "\n",
        "# 2. Finally install torch-geometric\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF-dupVzsOiN",
        "outputId": "394e443f-c758-4e8d-afdc-f08a6d223a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.9/750.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cpu\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "\n",
        "# Load your sequence\n",
        "sequence = np.load(\"casia_pose_sequence.npy\")  # shape: [79, 17, 2]\n",
        "\n",
        "# Prepare data for one sample\n",
        "frames, joints, coords = sequence.shape\n",
        "features = sequence.reshape(frames * joints, coords)  # [79*17, 2]\n",
        "\n",
        "# 1️⃣ Intra-frame edges (connect joints within each frame)\n",
        "edge_index = []\n",
        "for t in range(frames):\n",
        "    for i in range(joints):\n",
        "        for j in range(joints):\n",
        "            if i != j:\n",
        "                src = t * joints + i\n",
        "                dst = t * joints + j\n",
        "                edge_index.append([src, dst])\n",
        "\n",
        "# 2️⃣ Inter-frame temporal edges (connect same joints across frames)\n",
        "for t in range(frames - 1):\n",
        "    for j in range(joints):\n",
        "        src = t * joints + j\n",
        "        dst = (t + 1) * joints + j\n",
        "        edge_index.append([src, dst])\n",
        "        edge_index.append([dst, src])  # bidirectional\n",
        "\n",
        "# Final edge_index tensor\n",
        "edge_index = torch.tensor(edge_index).t().contiguous()  # shape: [2, num_edges]\n",
        "x = torch.tensor(features, dtype=torch.float)           # shape: [num_nodes, 2]\n",
        "y = torch.tensor([0])  # Label (e.g., class ID 0 for now)\n",
        "\n",
        "# Create graph\n",
        "graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
        "print(graph_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBQ_5kJXrwAw",
        "outputId": "5c0e0963-afa7-4113-b836-153642e36e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[1343, 2], edge_index=[2, 24140], y=[1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GaitGCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2, hidden_dim=64, num_classes=124):\n",
        "        super(GaitGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.lin = Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_dim]\n",
        "\n",
        "        return self.lin(x)\n"
      ],
      "metadata": {
        "id": "IwHFpjGdtAEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Example: if you have multiple graphs\n",
        "dataset = [graph_data]  # you’ll later append more samples here\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
      ],
      "metadata": {
        "id": "d4alKm6gtEXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GaitGCN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Simulate labels (e.g., subject 0)\n",
        "for data in dataset:\n",
        "    data.y = torch.tensor([0], dtype=torch.long)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P0FxqO5tLXP",
        "outputId": "07ed942c-544a-4ba0-c4c7-1ff9e79c2ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.9341\n",
            "Epoch 2, Loss: 7.0378\n",
            "Epoch 3, Loss: 6.2691\n",
            "Epoch 4, Loss: 5.4703\n",
            "Epoch 5, Loss: 4.6723\n",
            "Epoch 6, Loss: 3.9283\n",
            "Epoch 7, Loss: 3.1383\n",
            "Epoch 8, Loss: 2.4219\n",
            "Epoch 9, Loss: 1.6941\n",
            "Epoch 10, Loss: 1.1619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/HRNet-Human-Pose-Estimation /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "Ulc29FUbsjMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/HRNet_Human-Pose_Estimation\n"
      ],
      "metadata": {
        "id": "wIDWyzNPzpSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import mediapipe as mp\n",
        "\n",
        "# Initialize MediaPipe Pose once\n",
        "mp_pose = mp.solutions.pose\n",
        "pose_detector = mp_pose.Pose(static_image_mode=True)\n",
        "\n",
        "# Store processed graphs\n",
        "graph_dataset = []\n",
        "\n",
        "# === Set the path to your CASIA-B root ===\n",
        "ROOT_DIR = \"/content/drive/MyDrive/Dataset\"\n",
        "subject_folders = sorted(os.listdir(ROOT_DIR))\n",
        "\n",
        "def process_sequence(image_folder, subject_id):\n",
        "    image_paths = sorted(glob.glob(os.path.join(image_folder, \"*.png\")))\n",
        "    keypoints_seq = []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        result = pose_detector.process(rgb)\n",
        "        if result.pose_landmarks:\n",
        "            keypoints = [[l.x, l.y] for l in result.pose_landmarks.landmark[:17]]  # use first 17 joints\n",
        "            keypoints_seq.append(keypoints)\n",
        "\n",
        "    if len(keypoints_seq) < 2:\n",
        "        return None  # skip if not enough frames\n",
        "\n",
        "    sequence = np.array(keypoints_seq)  # [frames, 17, 2]\n",
        "    frames, joints, coords = sequence.shape\n",
        "\n",
        "    features = sequence.reshape(frames * joints, coords)\n",
        "\n",
        "    edge_index = []\n",
        "    for t in range(frames):\n",
        "        for i in range(joints):\n",
        "            for j in range(joints):\n",
        "                if i != j:\n",
        "                    edge_index.append([t * joints + i, t * joints + j])\n",
        "    for t in range(frames - 1):\n",
        "        for j in range(joints):\n",
        "            src = t * joints + j\n",
        "            dst = (t + 1) * joints + j\n",
        "            edge_index.append([src, dst])\n",
        "            edge_index.append([dst, src])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    y = torch.tensor([subject_id], dtype=torch.long)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# === Batch process all subjects ===\n",
        "for subject in subject_folders:\n",
        "    subject_path = os.path.join(ROOT_DIR, subject)\n",
        "    for condition in os.listdir(subject_path):\n",
        "        cond_path = os.path.join(subject_path, condition)\n",
        "        for view in os.listdir(cond_path):\n",
        "            seq_path = os.path.join(cond_path, view)\n",
        "            subject_id = int(subject)  # label = subject ID\n",
        "            graph = process_sequence(seq_path, subject_id)\n",
        "            if graph:\n",
        "                graph_dataset.append(graph)\n",
        "\n",
        "print(f\"✅ Loaded {len(graph_dataset)} graph samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "xxQdaQOU43OO",
        "outputId": "257b4847-4f3e-4315-9d5d-1419b084caaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mediapipe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-87b01edae2c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Initialize MediaPipe Pose once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}